This repository contains code for paper "ReinforceBug: A Framework to Generate Adversarial Textual Examples"
Adversarial Examples (AEs) generated by perturbingining examples are useful in improving the robustness of Deep Learning (DL) based models. 
Most prior works generate AEs that are either unconscionable due to lexical errors or semantically and functionally deviant from original examples.
In this paper, we present ReinforceBug, a reinforcement learning framework, that learns a policy that is transferable on unseen datasets and generates 
utility-preserving and transferable (on other models) AEs. Our experiments show that ReinforceBug is on average 10% more successful as compared to 
the state-of the-art attack TextFooler. Moreover, the target models have on average 73.64% confidence in wrong prediction, the generated AEs preserve the functional 
equivalence and semantic similarity (83.38%) to their original counterparts, and are transferable on other models with an average success rate of 46%


** The link to Google Colab **

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rWm1lFI8GiStbZlbroJ54n-H3NXP7OaG?usp=sharing]
